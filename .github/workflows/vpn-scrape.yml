name: Daily VPN Data Scrape & Sync

on:
  schedule:
    # Every day at 06:00 UTC
    - cron: "0 6 * * *"
  workflow_dispatch: # Allow manual trigger from GitHub Actions UI

jobs:
  # Tier 1: Top 8 VPNs (most popular, scrape daily)
  scrape-tier1:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    strategy:
      max-parallel: 3
      matrix:
        vpn:
          - nordvpn
          - surfshark
          - expressvpn
          - cyberghost
          - protonvpn
          - private-internet-access
          - mullvad
          - ipvanish

    steps:
      - name: Scrape ${{ matrix.vpn }} pricing
        run: |
          echo "Scraping ${{ matrix.vpn }}..."
          RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
            "${{ secrets.SITE_URL }}/api/pipeline/scrape" \
            -H "Content-Type: application/json" \
            -H "x-pipeline-key: ${{ secrets.PIPELINE_SECRET }}" \
            -d '{"type": "pricing", "vpnSlug": "${{ matrix.vpn }}"}' \
            --max-time 60)

          HTTP_CODE=$(echo "$RESPONSE" | tail -1)
          BODY=$(echo "$RESPONSE" | sed '$d')

          echo "Status: $HTTP_CODE"
          echo "$BODY" | head -c 300

          if [ "$HTTP_CODE" -ge 500 ]; then
            echo "::error::${{ matrix.vpn }} scrape failed with $HTTP_CODE"
          fi

  # Tier 2: Mid-tier VPNs (rotate daily â€” scrape ~10 per day)
  scrape-tier2:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Scrape tier 2 VPNs (rotating batch)
        run: |
          # All tier 2 VPN slugs
          ALL_VPNS=(
            vyprvpn tunnelbear windscribe hotspot-shield strongvpn
            purevpn atlas-vpn privatevpn torguard airvpn
            ivpn mozilla-vpn hide-me zenmate privadovpn
            hma astrill perfect-privacy goose-vpn trust-zone
            fastestvpn ovpn cactusvpn betternet speedify
            vpn-unlimited nordlayer perimeter-81 urban-vpn x-vpn
          )

          TOTAL=${#ALL_VPNS[@]}
          BATCH_SIZE=10
          # Use day-of-year to rotate which batch we scrape
          DAY=$(date +%j)
          START=$(( (DAY * BATCH_SIZE) % TOTAL ))

          echo "Day $DAY: scraping $BATCH_SIZE VPNs starting at index $START (of $TOTAL total)"

          for i in $(seq 0 $((BATCH_SIZE - 1))); do
            IDX=$(( (START + i) % TOTAL ))
            VPN=${ALL_VPNS[$IDX]}

            echo ""
            echo "=== Scraping $VPN ==="
            RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
              "${{ secrets.SITE_URL }}/api/pipeline/scrape" \
              -H "Content-Type: application/json" \
              -H "x-pipeline-key: ${{ secrets.PIPELINE_SECRET }}" \
              -d "{\"type\": \"pricing\", \"vpnSlug\": \"$VPN\"}" \
              --max-time 60)

            HTTP_CODE=$(echo "$RESPONSE" | tail -1)
            echo "Status: $HTTP_CODE"

            if [ "$HTTP_CODE" -ge 500 ]; then
              echo "::warning::$VPN scrape failed with $HTTP_CODE"
            fi

            # Small delay between requests
            sleep 2
          done

  scrape-countries:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    strategy:
      max-parallel: 2
      matrix:
        country:
          - iran
          - china
          - russia
          - uae
          - turkey

    steps:
      - name: Scrape ${{ matrix.country }} VPN/censorship data
        run: |
          echo "Scraping country data for ${{ matrix.country }}..."
          RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
            "${{ secrets.SITE_URL }}/api/pipeline/scrape" \
            -H "Content-Type: application/json" \
            -H "x-pipeline-key: ${{ secrets.PIPELINE_SECRET }}" \
            -d '{"type": "country-vpn", "countrySlug": "${{ matrix.country }}"}' \
            --max-time 90)

          HTTP_CODE=$(echo "$RESPONSE" | tail -1)
          BODY=$(echo "$RESPONSE" | sed '$d')

          echo "Status: $HTTP_CODE"
          echo "$BODY" | head -c 500

          if [ "$HTTP_CODE" -ge 500 ]; then
            echo "::warning::${{ matrix.country }} country scrape failed with $HTTP_CODE"
          fi

  scrape-news:
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Scrape VPN news
        run: |
          echo "Starting VPN news scrape..."
          RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
            "${{ secrets.SITE_URL }}/api/pipeline/scrape" \
            -H "Content-Type: application/json" \
            -H "x-pipeline-key: ${{ secrets.PIPELINE_SECRET }}" \
            -d '{"type": "news"}' \
            --max-time 120)

          HTTP_CODE=$(echo "$RESPONSE" | tail -1)
          BODY=$(echo "$RESPONSE" | sed '$d')

          echo "Status: $HTTP_CODE"
          echo "$BODY" | head -c 500

          if [ "$HTTP_CODE" -ge 500 ]; then
            echo "::error::VPN news scrape failed with $HTTP_CODE"
          fi

  sync-links:
    runs-on: ubuntu-latest
    timeout-minutes: 3

    steps:
      - name: Sync affiliate links
        run: |
          echo "Syncing Short.io affiliate links..."
          RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
            "${{ secrets.SITE_URL }}/api/pipeline/sync-links" \
            -H "Content-Type: application/json" \
            -H "x-pipeline-key: ${{ secrets.PIPELINE_SECRET }}" \
            -d '{}' \
            --max-time 60)

          HTTP_CODE=$(echo "$RESPONSE" | tail -1)
          BODY=$(echo "$RESPONSE" | sed '$d')

          echo "Status: $HTTP_CODE"
          echo "$BODY"

          if [ "$HTTP_CODE" -ge 400 ]; then
            echo "::warning::Affiliate link sync returned $HTTP_CODE"
          fi

  health-check:
    runs-on: ubuntu-latest
    needs: [scrape-tier1, scrape-tier2, scrape-countries, scrape-news, sync-links]
    if: always()
    timeout-minutes: 2

    steps:
      - name: Check pipeline health
        run: |
          echo "Checking pipeline health..."
          curl -s "${{ secrets.SITE_URL }}/api/pipeline/status" \
            -H "x-pipeline-key: ${{ secrets.PIPELINE_SECRET }}" \
            --max-time 30 | head -c 1000
