name: Daily VPN Data Scrape & Sync

on:
  schedule:
    # Every day at 06:00 UTC
    - cron: "0 6 * * *"
  workflow_dispatch: # Allow manual trigger from GitHub Actions UI

jobs:
  scrape-and-sync:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    strategy:
      max-parallel: 2
      matrix:
        vpn:
          - nordvpn
          - surfshark
          - expressvpn
          - cyberghost
          - protonvpn
          - private-internet-access
          - mullvad
          - ipvanish

    steps:
      - name: Scrape ${{ matrix.vpn }} pricing
        run: |
          echo "Scraping ${{ matrix.vpn }}..."
          RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
            "${{ secrets.SITE_URL }}/api/pipeline/scrape" \
            -H "Content-Type: application/json" \
            -H "x-pipeline-key: ${{ secrets.PIPELINE_SECRET }}" \
            -d '{"type": "pricing", "vpnSlug": "${{ matrix.vpn }}"}' \
            --max-time 60)

          HTTP_CODE=$(echo "$RESPONSE" | tail -1)
          BODY=$(echo "$RESPONSE" | sed '$d')

          echo "Status: $HTTP_CODE"
          echo "$BODY" | head -c 300

          if [ "$HTTP_CODE" -ge 500 ]; then
            echo "::error::${{ matrix.vpn }} scrape failed with $HTTP_CODE"
          fi

  scrape-news:
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Scrape VPN news
        run: |
          echo "Starting VPN news scrape..."
          RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
            "${{ secrets.SITE_URL }}/api/pipeline/scrape" \
            -H "Content-Type: application/json" \
            -H "x-pipeline-key: ${{ secrets.PIPELINE_SECRET }}" \
            -d '{"type": "news"}' \
            --max-time 120)

          HTTP_CODE=$(echo "$RESPONSE" | tail -1)
          BODY=$(echo "$RESPONSE" | sed '$d')

          echo "Status: $HTTP_CODE"
          echo "$BODY" | head -c 500

          if [ "$HTTP_CODE" -ge 500 ]; then
            echo "::error::VPN news scrape failed with $HTTP_CODE"
          fi

  sync-links:
    runs-on: ubuntu-latest
    timeout-minutes: 3

    steps:
      - name: Sync affiliate links
        run: |
          echo "Syncing Short.io affiliate links..."
          RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
            "${{ secrets.SITE_URL }}/api/pipeline/sync-links" \
            -H "Content-Type: application/json" \
            -H "x-pipeline-key: ${{ secrets.PIPELINE_SECRET }}" \
            -d '{}' \
            --max-time 60)

          HTTP_CODE=$(echo "$RESPONSE" | tail -1)
          BODY=$(echo "$RESPONSE" | sed '$d')

          echo "Status: $HTTP_CODE"
          echo "$BODY"

          if [ "$HTTP_CODE" -ge 400 ]; then
            echo "::warning::Affiliate link sync returned $HTTP_CODE"
          fi

  health-check:
    runs-on: ubuntu-latest
    needs: [scrape-and-sync, scrape-news, sync-links]
    if: always()
    timeout-minutes: 2

    steps:
      - name: Check pipeline health
        run: |
          echo "Checking pipeline health..."
          curl -s "${{ secrets.SITE_URL }}/api/pipeline/status" \
            -H "x-pipeline-key: ${{ secrets.PIPELINE_SECRET }}" \
            --max-time 30 | head -c 1000
